{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting testfixtures\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/38/6a885903ede5e7155665b7d2a3a6fa9416df3c90498c6d3d68103a972a17/testfixtures-6.10.0-py2.py3-none-any.whl (86kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 989kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: testfixtures\n",
      "Successfully installed testfixtures-6.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install testfixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in /anaconda3/lib/python3.6/site-packages (0.9.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 8.11 µs\n",
      "IMDB archive directory already available without download.\n",
      "Cleaning up dataset...\n",
      " train/pos: 12500 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train/neg: 12500 files\n",
      " test/pos: 12500 files\n",
      " test/neg: 12500 files\n",
      " train/unsup: 50000 files\n",
      "Success, alldata-id.txt is available for next steps.\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "\n",
    "import locale\n",
    "import glob\n",
    "import os.path\n",
    "import requests\n",
    "import tarfile\n",
    "import sys\n",
    "import codecs\n",
    "from smart_open import smart_open\n",
    "import re\n",
    "\n",
    "dirname = 'aclImdb'\n",
    "filename = 'aclImdb_v1.tar.gz'\n",
    "locale.setlocale(locale.LC_ALL, 'C')\n",
    "all_lines = []\n",
    "\n",
    "if sys.version > '3':\n",
    "    control_chars = [chr(0x85)]\n",
    "else:\n",
    "    control_chars = [unichr(0x85)]\n",
    "\n",
    "# Convert text to lower-case and strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    norm_text = re.sub(r\"([\\.\\\",\\(\\)!\\?;:])\", \" \\\\1 \", norm_text)\n",
    "    return norm_text\n",
    "\n",
    "if not os.path.isfile('aclImdb/alldata-id.txt'):\n",
    "    if not os.path.isdir(dirname):\n",
    "        if not os.path.isfile(filename):\n",
    "            # Download IMDB archive\n",
    "            print(\"Downloading IMDB archive...\")\n",
    "            url = u'http://ai.stanford.edu/~amaas/data/sentiment/' + filename\n",
    "            r = requests.get(url)\n",
    "            with smart_open(filename, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        # if error here, try `tar xfz aclImdb_v1.tar.gz` outside notebook, then re-run this cell\n",
    "        tar = tarfile.open(filename, mode='r')\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "    else:\n",
    "        print(\"IMDB archive directory already available without download.\")\n",
    "\n",
    "    # Collect & normalize test/train data\n",
    "    print(\"Cleaning up dataset...\")\n",
    "    folders = ['train/pos', 'train/neg', 'test/pos', 'test/neg', 'train/unsup']\n",
    "    for fol in folders:\n",
    "        temp = u''\n",
    "        newline = \"\\n\".encode(\"utf-8\")\n",
    "        output = fol.replace('/', '-') + '.txt'\n",
    "        # Is there a better pattern to use?\n",
    "        txt_files = glob.glob(os.path.join(dirname, fol, '*.txt'))\n",
    "        print(\" %s: %i files\" % (fol, len(txt_files)))\n",
    "        with smart_open(os.path.join(dirname, output), \"wb\") as n:\n",
    "            for i, txt in enumerate(txt_files):\n",
    "                with smart_open(txt, \"rb\") as t:\n",
    "                    one_text = t.read().decode(\"utf-8\")\n",
    "                    for c in control_chars:\n",
    "                        one_text = one_text.replace(c, ' ')\n",
    "                    one_text = normalize_text(one_text)\n",
    "                    all_lines.append(one_text)\n",
    "                    n.write(one_text.encode(\"utf-8\"))\n",
    "                    n.write(newline)\n",
    "\n",
    "    # Save to disk for instant re-use on any future runs\n",
    "    with smart_open(os.path.join(dirname, 'alldata-id.txt'), 'wb') as f:\n",
    "        for idx, line in enumerate(all_lines):\n",
    "            num_line = u\"_*{0} {1}\\n\".format(idx, line)\n",
    "            f.write(num_line.encode(\"utf-8\"))\n",
    "\n",
    "assert os.path.isfile(\"aclImdb/alldata-id.txt\"), \"alldata-id.txt unavailable\"\n",
    "print(\"Success, alldata-id.txt is available for next steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 docs: 25000 train-sentiment, 25000 test-sentiment\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "\n",
    "# this data object class suffices as a `TaggedDocument` (with `words` and `tags`) \n",
    "# plus adds other state helpful for our later evaluation/reporting\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "\n",
    "alldocs = []\n",
    "with smart_open('aclImdb/alldata-id.txt', 'rb', encoding='utf-8') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        tokens = gensim.utils.to_unicode(line).split()\n",
    "        words = tokens[1:]\n",
    "        tags = [line_no] # 'tags = [tokens[0]]' would also work at extra memory cost\n",
    "        split = ['train', 'test', 'extra', 'extra'][line_no//25000]  # 25k train, 25k test, 25k extra\n",
    "        sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no//12500] # [12.5K pos, 12.5K neg]*2 then unknown\n",
    "        alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
    "\n",
    "train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
    "\n",
    "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(alldocs), len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "doc_list = alldocs[:]  \n",
    "shuffle(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.96 µs\n",
      "Doc2Vec(dbow,d100,n5,mc2,t8) vocabulary scanned & state initialized\n",
      "Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8) vocabulary scanned & state initialized\n",
      "Doc2Vec(dm/c,d100,n5,w5,mc2,t8) vocabulary scanned & state initialized\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "simple_models = [\n",
    "    # PV-DBOW plain\n",
    "    Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=20, workers=cores),\n",
    "    # PV-DM w/ default averaging; a higher starting alpha may improve CBOW/PV-DM modes\n",
    "    Doc2Vec(dm=1, vector_size=100, window=10, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=20, workers=cores, alpha=0.05, comment='alpha=0.05'),\n",
    "    # PV-DM w/ concatenation - big, slow, experimental mode\n",
    "    # window=5 (both sides) approximates paper's apparent 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, vector_size=100, window=5, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=20, workers=cores),\n",
    "]\n",
    "\n",
    "for model in simple_models:\n",
    "    model.build_vocab(alldocs)\n",
    "    print(\"%s vocabulary scanned & state initialized\" % model)\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[1]])\n",
    "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from random import sample\n",
    "    \n",
    "def logistic_predictor_from_data(train_targets, train_regressors):\n",
    "    \"\"\"Fit a statsmodel logistic predictor on supplied data\"\"\"\n",
    "    logit = sm.Logit(train_targets, train_regressors)\n",
    "    predictor = logit.fit(disp=0)\n",
    "    # print(predictor.summary())\n",
    "    return predictor\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, \n",
    "                         reinfer_train=False, reinfer_test=False, \n",
    "                         infer_steps=None, infer_alpha=None, infer_subsample=0.2):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets = [doc.sentiment for doc in train_set]\n",
    "    if reinfer_train:\n",
    "        train_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in train_set]\n",
    "    else:\n",
    "        train_regressors = [test_model.docvecs[doc.tags[0]] for doc in train_set]\n",
    "    train_regressors = sm.add_constant(train_regressors)\n",
    "    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
    "\n",
    "    test_data = test_set\n",
    "    if reinfer_test:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
    "    test_regressors = sm.add_constant(test_regressors)\n",
    "    \n",
    "    # Predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_data])\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "error_rates = defaultdict(lambda: 1.0)  # To selectively print only best errors achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Doc2Vec(dbow,d100,n5,mc2,t8)\n",
      "CPU times: user 20min 22s, sys: 37.9 s, total: 21min\n",
      "Wall time: 5min 52s\n",
      "\n",
      "Evaluating Doc2Vec(dbow,d100,n5,mc2,t8)\n",
      "CPU times: user 2.31 s, sys: 236 ms, total: 2.54 s\n",
      "Wall time: 794 ms\n",
      "\n",
      "0.101360 Doc2Vec(dbow,d100,n5,mc2,t8)\n",
      "\n",
      "Training Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)\n",
      "CPU times: user 33min 31s, sys: 1min 41s, total: 35min 13s\n",
      "Wall time: 8min 59s\n",
      "\n",
      "Evaluating Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)\n",
      "CPU times: user 2 s, sys: 223 ms, total: 2.23 s\n",
      "Wall time: 692 ms\n",
      "\n",
      "0.153600 Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)\n",
      "\n",
      "Training Doc2Vec(dm/c,d100,n5,w5,mc2,t8)\n",
      "CPU times: user 1h 11min 38s, sys: 47.7 s, total: 1h 12min 26s\n",
      "Wall time: 30min 33s\n",
      "\n",
      "Evaluating Doc2Vec(dm/c,d100,n5,w5,mc2,t8)\n",
      "CPU times: user 2.02 s, sys: 264 ms, total: 2.29 s\n",
      "Wall time: 740 ms\n",
      "\n",
      "0.223560 Doc2Vec(dm/c,d100,n5,w5,mc2,t8)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in simple_models: \n",
    "    print(\"Training %s\" % model)\n",
    "    %time model.train(doc_list, total_examples=len(doc_list), epochs=model.epochs)\n",
    "    \n",
    "    print(\"\\nEvaluating %s\" % model)\n",
    "    %time err_rate, err_count, test_count, predictor = error_rate_for_model(model, train_docs, test_docs)\n",
    "    error_rates[str(model)] = err_rate\n",
    "    print(\"\\n%f %s\\n\" % (err_rate, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)\n",
      "CPU times: user 4.44 s, sys: 928 ms, total: 5.37 s\n",
      "Wall time: 1.6 s\n",
      "\n",
      "0.103200 Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)\n",
      "\n",
      "\n",
      "Evaluating Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(dm/c,d100,n5,w5,mc2,t8)\n",
      "CPU times: user 4.79 s, sys: 1.04 s, total: 5.82 s\n",
      "Wall time: 1.55 s\n",
      "\n",
      "0.102360 Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(dm/c,d100,n5,w5,mc2,t8)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in [models_by_name['dbow+dmm'], models_by_name['dbow+dmc']]: \n",
    "    print(\"\\nEvaluating %s\" % model)\n",
    "    %time err_rate, err_count, test_count, predictor = error_rate_for_model(model, train_docs, test_docs)\n",
    "    error_rates[str(model)] = err_rate\n",
    "    print(\"\\n%f %s\\n\" % (err_rate, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Err_rate Model\n",
      "0.101360 Doc2Vec(dbow,d100,n5,mc2,t8)\n",
      "0.102360 Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(dm/c,d100,n5,w5,mc2,t8)\n",
      "0.103200 Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)\n",
      "0.153600 Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)\n",
      "0.223560 Doc2Vec(dm/c,d100,n5,w5,mc2,t8)\n"
     ]
    }
   ],
   "source": [
    "# Compare error rates achieved, best-to-worst\n",
    "print(\"Err_rate Model\")\n",
    "for rate, name in sorted((rate, name) for name, rate in error_rates.items()):\n",
    "    print(\"%f %s\" % (rate, name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for doc 21677...\n",
      "Doc2Vec(dbow,d100,n5,mc2,t8):\n",
      " [(21677, 0.959844708442688), (77987, 0.562900722026825), (49050, 0.5591022372245789)]\n",
      "Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8):\n",
      " [(21677, 0.9112704992294312), (90740, 0.5546159148216248), (15028, 0.5322123765945435)]\n",
      "Doc2Vec(dm/c,d100,n5,w5,mc2,t8):\n",
      " [(21677, 0.9156535267829895), (27290, 0.4557366371154785), (33260, 0.4514159560203552)]\n"
     ]
    }
   ],
   "source": [
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # Pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "for model in simple_models:\n",
    "    inferred_docvec = model.infer_vector(alldocs[doc_id].words)\n",
    "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (94775): «this was a great flick . i love the donald sutherland character . kind of reminds you of his role in invasion of the body snatchers . i saw this movie when it was released and have been looking for a copy of it on dvd since then ! right now it is only available on vhs .»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8):\n",
      "\n",
      "MOST (35409, 0.8156343102455139): «brilliant and moving performances by tom courtenay and peter finch .»\n",
      "\n",
      "MEDIAN (96897, 0.4166574478149414): «i don't know what type of kool-aid everyone else has been drinking , but this movie was a total disappointment . brilliant ? huh ? are we watching the same movie ? because the movie i saw had a fairly odious protagonist , a supporting cast that existed solely to be ridiculed , a series of 'crazy' events befalling our 'hapless' protagonist that exist nowhere on the reality continuum , and seemed to have been invented by a corp committee composed of former frat 'dudes' pitching ideas for 'worst night ever' that aren't funny and mostly don't even make sense . most ( all ? ) of the 'crazy' people whom whatever the guys name is meets in his 'romp' thru nyc are women , who proceed to beguile him with their alluring sex appeal ( with the exception of poor teri garr , who for some reason is made out to be some pathetic spinster . . . or something . . . ) only to send him running once they reveal their true 'scary' woman-personalities . isn't that always the way of it ? ? this movie is sexist , stupid , unlikeable , unfunny and makes little to no sense . madcap is one thing . this is more like nocap . i've always thought scorcese was overrated , especially considering all of the movies he's made in the past ten years have sucked besides the departed , and that honestly wasn't even that great . don't even bother with the trailer for this one»\n",
      "\n",
      "LEAST (44272, 0.0748508870601654): «primary plot ! primary direction ! poor interpretation .»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
    "model = random.choice(simple_models)  # and a random model\n",
    "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
    "print(u'TARGET (%d): «%s»\\n' % (doc_id, ' '.join(alldocs[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(alldocs[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_models = simple_models[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most similar words for 'waqt' (19 occurences)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Doc2Vec(dbow,d100,n5,mc2,t8)</th><th>Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)</th><th>Doc2Vec(dm/c,d100,n5,w5,mc2,t8)</th></tr><tr><td>[('puzzlers', 0.4229496717453003),<br>\n",
       "(\"pedestrian's\", 0.4225466251373291),<br>\n",
       "('gilded', 0.4051579535007477),<br>\n",
       "(\"york'is\", 0.4004163146018982),<br>\n",
       "(\"shaft's\", 0.3847389221191406),<br>\n",
       "('issacs', 0.3838694393634796),<br>\n",
       "('buds--and', 0.37943464517593384),<br>\n",
       "('-5', 0.3792581260204315),<br>\n",
       "('sundowners', 0.3786245882511139),<br>\n",
       "('addle-brained', 0.37728971242904663),<br>\n",
       "('cloak', 0.3693162500858307),<br>\n",
       "('godhood', 0.36655810475349426),<br>\n",
       "('temple', 0.3651615083217621),<br>\n",
       "('olde', 0.36450743675231934),<br>\n",
       "('innate', 0.3610934615135193),<br>\n",
       "('über', 0.3605837821960449),<br>\n",
       "('renoir', 0.3604622483253479),<br>\n",
       "('windshields', 0.36016377806663513),<br>\n",
       "('spectacles', 0.35989752411842346),<br>\n",
       "('curitz', 0.3594284951686859)]</td><td>[('_full', 0.5873367786407471),<br>\n",
       "(\"'2001\", 0.577534019947052),<br>\n",
       "('farscape', 0.567756175994873),<br>\n",
       "('jodhaa-akbar', 0.563327431678772),<br>\n",
       "('wormhole', 0.5610390901565552),<br>\n",
       "('hex', 0.5601495504379272),<br>\n",
       "('ddlj', 0.5501421689987183),<br>\n",
       "('-zoom', 0.5493675470352173),<br>\n",
       "('dyrl', 0.5453404188156128),<br>\n",
       "('commedia', 0.5445129871368408),<br>\n",
       "('bb', 0.5419938564300537),<br>\n",
       "('kryten', 0.5395208597183228),<br>\n",
       "('spaceland', 0.5364852547645569),<br>\n",
       "(\"dell'arte\", 0.5360150337219238),<br>\n",
       "('byword', 0.5348269939422607),<br>\n",
       "('joystick', 0.5329856872558594),<br>\n",
       "(\"'2009\", 0.5298449993133545),<br>\n",
       "('*reason*', 0.5264933109283447),<br>\n",
       "('alfalfa', 0.5256202220916748),<br>\n",
       "('self-composed', 0.523798942565918)]</td><td>[('hddcs', 0.5533608198165894),<br>\n",
       "('eklavya', 0.5529837608337402),<br>\n",
       "('ju', 0.5513511896133423),<br>\n",
       "('scheherazade', 0.5221931338310242),<br>\n",
       "('jamel', 0.5171080827713013),<br>\n",
       "('pakeezah', 0.5130993127822876),<br>\n",
       "('kaurismaki', 0.5128692388534546),<br>\n",
       "('kabuto', 0.5099593997001648),<br>\n",
       "('frederik', 0.500555157661438),<br>\n",
       "(\"l'iceberg\", 0.49905508756637573),<br>\n",
       "(\"'nosferatu'\", 0.4987587630748749),<br>\n",
       "('yann', 0.4960111975669861),<br>\n",
       "('palm-reading', 0.4932093620300293),<br>\n",
       "('sahni', 0.49007648229599),<br>\n",
       "('startrek', 0.48972398042678833),<br>\n",
       "('gustave', 0.48845726251602173),<br>\n",
       "('sayonara', 0.4883417785167694),<br>\n",
       "('overdosing', 0.48698553442955017),<br>\n",
       "('rock-a-doodle', 0.4864930212497711),<br>\n",
       "('mileena', 0.4835045635700226)]</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from IPython.display import HTML\n",
    "# pick a random word with a suitable number of occurences\n",
    "while True:\n",
    "    word = random.choice(word_models[0].wv.index2word)\n",
    "    if word_models[0].wv.vocab[word].count > 10:\n",
    "        break\n",
    "# or uncomment below line, to just pick a word from the relevant domain:\n",
    "#word = 'comedy/drama'\n",
    "similars_per_model = [str(model.wv.most_similar(word, topn=20)).replace('), ','),<br>\\n') for model in word_models]\n",
    "similar_table = (\"<table><tr><th>\" +\n",
    "    \"</th><th>\".join([str(model) for model in word_models]) + \n",
    "    \"</th></tr><tr><td>\" +\n",
    "    \"</td><td>\".join(similars_per_model) +\n",
    "    \"</td></tr></table>\")\n",
    "print(\"most similar words for '%s' (%d occurences)\" % (word, simple_models[0].wv.vocab[word].count))\n",
    "HTML(similar_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading analogy questions file...\n",
      "Success, questions-words.txt is available for next steps.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "# grab the file if not already local\n",
    "questions_filename = 'questions-words.txt'\n",
    "if not os.path.isfile(questions_filename):\n",
    "    # Download IMDB archive\n",
    "    print(\"Downloading analogy questions file...\")\n",
    "    url = u'https://raw.githubusercontent.com/tmikolov/word2vec/master/questions-words.txt'\n",
    "    r = requests.get(url)\n",
    "    with smart_open(questions_filename, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "assert os.path.isfile(questions_filename), \"questions-words.txt unavailable\"\n",
    "print(\"Success, questions-words.txt is available for next steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow,d100,n5,mc2,t8): 0.00% correct (0 of 14657)\n",
      "Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8): 16.72% correct (2451 of 14657)\n",
      "Doc2Vec(dm/c,d100,n5,w5,mc2,t8): 18.89% correct (2769 of 14657)\n"
     ]
    }
   ],
   "source": [
    "# Note: this analysis takes many minutes\n",
    "for model in word_models:\n",
    "    score, sections = model.wv.evaluate_word_analogies('questions-words.txt')\n",
    "    correct, incorrect = len(sections[-1]['correct']), len(sections[-1]['incorrect'])\n",
    "    print('%s: %0.2f%% correct (%d of %d)' % (model, float(correct*100)/(correct+incorrect), correct, correct+incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Doc2Vec(dbow,d100,n5,mc2,t8) re-inferred\n",
      "CPU times: user 6min 29s, sys: 1.59 s, total: 6min 31s\n",
      "Wall time: 6min 30s\n",
      "\n",
      "0.102840 Doc2Vec(dbow,d100,n5,mc2,t8)_reinferred\n",
      "\n",
      "Evaluating Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8) re-inferred\n",
      "CPU times: user 8min 53s, sys: 2.61 s, total: 8min 56s\n",
      "Wall time: 13min 31s\n",
      "\n",
      "0.145960 Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)_reinferred\n",
      "\n",
      "Evaluating Doc2Vec(dm/c,d100,n5,w5,mc2,t8) re-inferred\n",
      "CPU times: user 14min 28s, sys: 2.57 s, total: 14min 31s\n",
      "Wall time: 14min 31s\n",
      "\n",
      "0.216600 Doc2Vec(dm/c,d100,n5,w5,mc2,t8)_reinferred\n",
      "\n",
      "Evaluating Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8) re-inferred\n",
      "CPU times: user 15min 42s, sys: 3.63 s, total: 15min 46s\n",
      "Wall time: 24min 46s\n",
      "\n",
      "0.104080 Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)_reinferred\n",
      "\n",
      "Evaluating Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(dm/c,d100,n5,w5,mc2,t8) re-inferred\n",
      "CPU times: user 20min 56s, sys: 3.97 s, total: 21min\n",
      "Wall time: 31min 22s\n",
      "\n",
      "0.102920 Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(dm/c,d100,n5,w5,mc2,t8)_reinferred\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in simple_models + [models_by_name['dbow+dmm'], models_by_name['dbow+dmc']]: \n",
    "    print(\"Evaluating %s re-inferred\" % str(model))\n",
    "    pseudomodel_name = str(model)+\"_reinferred\"\n",
    "    %time err_rate, err_count, test_count, predictor = error_rate_for_model(model, train_docs, test_docs, reinfer_train=True, reinfer_test=True, infer_subsample=1.0)\n",
    "    error_rates[pseudomodel_name] = err_rate\n",
    "    print(\"\\n%f %s\\n\" % (err_rate, pseudomodel_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare error rates achieved, best-to-worst\n",
    "print(\"Err_rate Model\")\n",
    "for rate, name in sorted((rate, name) for name, rate in error_rates.items()):\n",
    "    print(\"%f %s\" % (rate, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "rootLogger = logging.getLogger()\n",
    "rootLogger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
